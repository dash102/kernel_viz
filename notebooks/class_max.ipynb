{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neuron_visualization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b155603f7a8411e87db613386e5563b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f63019a4764140369fd69ab534891474",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c8c432ef077476cbc986403b37789e6",
              "IPY_MODEL_bdd9a866d94e405090fa28700a2ac48d"
            ]
          }
        },
        "f63019a4764140369fd69ab534891474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c8c432ef077476cbc986403b37789e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a2c3a8025af94a93b82af30bfa2edc44",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30beedb2df114a328c98227560f2451b"
          }
        },
        "bdd9a866d94e405090fa28700a2ac48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e8b2f1dd8188469cbab8de64afd7ebfa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 58.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2016bfa018f1418088d69639d471c31d"
          }
        },
        "a2c3a8025af94a93b82af30bfa2edc44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30beedb2df114a328c98227560f2451b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8b2f1dd8188469cbab8de64afd7ebfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2016bfa018f1418088d69639d471c31d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5QJc-ecuKOW"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "# needed to add for some weird jupyter notebook thing\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from numpy.random import default_rng\n",
        "import copy\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1HCWItuKOd"
      },
      "source": [
        "### Setting seeds for repeatability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pof1mq_5uKOd"
      },
      "source": [
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE8JUUcDuKOe"
      },
      "source": [
        "### Defining model and extracting feature maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1mNZpnKuKOe"
      },
      "source": [
        "StepImage basically does gradient updates to try to move images in the direction of a neuron's gradient\n",
        "\n",
        "It's used to maximize a specific neuron's response to an image (for example, maximizing the response of a neuron looking for cat faces). Practically in our project, it could be used as an interpretability tool; people don't know what specific neurons are looking for bc deep networks are hard to undrstand, so if you morph an image to move in the direction of the patterns that a specific neuron is biased towards, you can see more clearly what that neuron is looking for.\n",
        "\n",
        "Credit: written by 6.869 staff as code for a pset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzwc1euhuKOe"
      },
      "source": [
        "class StepImage():\n",
        "    def __init__(self, orig_input, step_size=2, is_normalized=True, \n",
        "               renorm=True, eps=30, norm_update='l2'):\n",
        "        self.orig_input = orig_input\n",
        "        if is_normalized:\n",
        "            mean=[0.485, 0.456, 0.406]\n",
        "            std= [0.229, 0.224, 0.225]\n",
        "        else:\n",
        "            mean=[0., 0., 0.]\n",
        "            std= [1., 1., 1.]\n",
        "\n",
        "        is_cuda = orig_input.is_cuda\n",
        "        self.mean = torch.tensor(mean)[:, None, None]\n",
        "        self.std = torch.tensor(std)[:, None, None]\n",
        "        if is_cuda:\n",
        "            self.mean = self.mean.cuda()\n",
        "            self.std = self.std.cuda()\n",
        "        self.eps = eps\n",
        "        self.renorm = renorm\n",
        "        self.step_size = step_size\n",
        "        self.norm_update = norm_update\n",
        "\n",
        "    def project(self, x):\n",
        "        diff = x - self.orig_input\n",
        "        if self.renorm:\n",
        "            diff = diff.renorm(p=2, dim=0, maxnorm=self.eps)\n",
        "        val_projected = self.orig_input + diff\n",
        "\n",
        "        val_projected *= self.std\n",
        "        val_projected += self.mean\n",
        "        val_clamped = torch.clamp(val_projected, 0, 1)\n",
        "        val_clamped -= self.mean\n",
        "        val_clamped /= self.std\n",
        "        return val_clamped\n",
        "\n",
        "    # move one step in the direction of the neuron gradient\n",
        "    def step(self, x, g):\n",
        "        step_size = self.step_size\n",
        "        # Scale g so that each element of the batch is at least norm 1\n",
        "        if self.norm_update == 'l2':\n",
        "            l = len(x.shape) - 1\n",
        "            g_norm = torch.norm(g.view(g.shape[0], -1), dim=1).view(-1, *([1]*l))\n",
        "        else:\n",
        "            g_norm = torch.torch.abs(g).mean()\n",
        "        scaled_g = g / (g_norm + 1e-10)\n",
        "        stepped = x + scaled_g * step_size\n",
        "        projected = self.project(stepped)\n",
        "        return projected"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNhx2k-2uKOf"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "def prepare_image(image_cv2, do_normalize=True):\n",
        "    # Resize\n",
        "    img = cv2.resize(image_cv2, (224, 224))\n",
        "    img = img[:, :, ::-1].copy()\n",
        "    # Convert to tensor\n",
        "    tensor_img = transforms.functional.to_tensor(img)\n",
        "\n",
        "    # Possibly normalize\n",
        "    if do_normalize:\n",
        "        tensor_img = normalize(tensor_img)\n",
        "    # Put image in a batch\n",
        "    batch_tensor_img = torch.unsqueeze(tensor_img, 0)\n",
        "  \n",
        "    # Put the image in the gpu\n",
        "    if cuda_available:\n",
        "        batch_tensor_img = batch_tensor_img.cuda()\n",
        "    return batch_tensor_img\n",
        "\n",
        "def UnNormalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]):\n",
        "    std_arr = torch.tensor(std)[:, None, None]\n",
        "    mean_arr = torch.tensor(mean)[:, None, None]\n",
        "    def func(img):\n",
        "        img = img.clone()\n",
        "        img *= std_arr\n",
        "        img += mean_arr\n",
        "        return img\n",
        "    return func\n",
        "\n",
        "unnormalize = UnNormalize()\n",
        "\n",
        "def obtain_image(tensor_img, do_normalize=True):\n",
        "    tensor_img = tensor_img.cpu()\n",
        "    if do_normalize:\n",
        "        tensor_img = unnormalize(tensor_img)\n",
        "    img = transforms.functional.to_pil_image((tensor_img.data))\n",
        "    return img"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztwVy9IXuKOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "5b155603f7a8411e87db613386e5563b",
            "f63019a4764140369fd69ab534891474",
            "0c8c432ef077476cbc986403b37789e6",
            "bdd9a866d94e405090fa28700a2ac48d",
            "a2c3a8025af94a93b82af30bfa2edc44",
            "30beedb2df114a328c98227560f2451b",
            "e8b2f1dd8188469cbab8de64afd7ebfa",
            "2016bfa018f1418088d69639d471c31d"
          ]
        },
        "outputId": "598005cf-da7d-4c35-b496-a91c0d6906db"
      },
      "source": [
        "# change between true and false to simulate what neurons are biased towards before training and after training\n",
        "# we expect the results to be much more structured with a trained model\n",
        "net = models.resnet50(pretrained=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b155603f7a8411e87db613386e5563b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "QCxR0NI9uKOg",
        "outputId": "a5b03f9b-5dd5-4889-8ba3-84d147565b76"
      },
      "source": [
        "# this is our image that we're gonna perturb\n",
        "# I chose a pure-white image bc it seems like results are decent with this\n",
        "# but feel free to change if you find a better starting image\n",
        "\n",
        "# img = torch.zeros((1, 3, 224, 224))\n",
        "# img = torch.rand((1, 3, 224, 224))\n",
        "img = torch.ones((1, 3, 224, 224))\n",
        "plt.imshow(img[0].permute(1, 2, 0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7195b2df90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnuU-SMA3niQ",
        "outputId": "12c69313-7681-4cba-f69b-85dd4f92be98"
      },
      "source": [
        "net"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rnIOfRAuKOg"
      },
      "source": [
        "# modify to show different layers and channels\n",
        "# there isn't really any semantic relationship between different channels like there is with different layers, \n",
        "# so we can just pick a couple of random channels per layer to show\n",
        "\n",
        "# we expect outputs to look more meta (less locally-regular) as layer_id increases\n",
        "layer_id = 5\n",
        "# 207\n",
        "# 889\n",
        "channel_id = 889"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwhDAQLRuKOh"
      },
      "source": [
        "# This function creates a function that gives the output of a given\n",
        "# network at layer: layer_id.\n",
        "\n",
        "def model_layer(model, layer_id):\n",
        "    # These are the 4 sequential layers of resnet50\n",
        "    layers = [model.layer1, model.layer2, model.layer3, model.layer4, model.avgpool, model.fc]  \n",
        "    def forward(input):\n",
        "        layers_used = layers[:(layer_id+1)]\n",
        "        x = input\n",
        "        x = model.conv1(x)\n",
        "        x = model.bn1(x)\n",
        "        x = model.relu(x)\n",
        "        x = model.maxpool(x)\n",
        "        for l in layers_used:\n",
        "          if isinstance(l, nn.Linear) and layer_id == 5:\n",
        "            # print(x.shape)\n",
        "            num_channels, height, width, _ = x.shape\n",
        "            x = x.view(num_channels, (height * width))\n",
        "            # print(x.shape)\n",
        "          x = l(x)\n",
        "        return x\n",
        "    return forward"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agWlQvWquKOh",
        "outputId": "903f071e-d3c7-462a-9ceb-08ea24d0a32f"
      },
      "source": [
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# batch_tensor = img.clone().requires_grad_(True)\n",
        "step = StepImage(img, step_size=0.05, renorm=False, norm_update='abs', is_normalized=False)\n",
        "\n",
        "net_l = model_layer(net, layer_id)\n",
        "\n",
        "for _ in tqdm(range(1000)):\n",
        "    logit = net_l(batch_tensor)\n",
        "    \n",
        "    loss = torch.norm(logit[0,channel_id,...], p=2)\n",
        "    gradient, = torch.autograd.grad(loss, batch_tensor)\n",
        "    batch_tensor = step.step(batch_tensor, gradient)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 77/1000 [00:39<07:53,  1.95it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-OnP5FMuKOi"
      },
      "source": [
        "original_image = obtain_image(img[0, :], do_normalize=False)\n",
        "modified_image = obtain_image(batch_tensor[0, :], do_normalize=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.set_title(f'Modified image (layer {layer_id}, channel {channel_id})')\n",
        "ax.imshow(modified_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yKt5y6YvbKY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}